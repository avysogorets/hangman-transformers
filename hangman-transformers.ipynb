{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/amv458/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device cuda:0 is ready]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'[device {device} is ready]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionaries(train_file=\"words_250000_train.txt\"):\n",
    "    text_file = open(train_file, \"r\")\n",
    "    train_dictionary = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    train_set = set(train_dictionary)\n",
    "    test_dictionary = []\n",
    "    for synset in wordnet.all_synsets():\n",
    "        for lemma in synset.lemmas():\n",
    "            word = lemma.name().lower()\n",
    "            if word not in train_set and word.isalpha() :\n",
    "                test_dictionary.append(word)\n",
    "    return train_dictionary, test_dictionary\n",
    "\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\" Average pooling layer for the Transformer.\n",
    "    \"\"\"\n",
    "    token_embeddings = token_embeddings.to(attention_mask.device)\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).float())\n",
    "    sum_embeddings=torch.sum(token_embeddings * input_mask_expanded, -2)\n",
    "    sum_mask=torch.clamp(input_mask_expanded.sum(-2), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class HangmanTransformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "            max_seq_len,\n",
    "            model_dim,\n",
    "            latent_dim,\n",
    "            tokenizer,\n",
    "            device=torch.device('cpu'),\n",
    "            num_heads=1,\n",
    "            num_layers=1,\n",
    "            vocab_size=28):\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        bert.config.hidden_size=model_dim\n",
    "        bert.config.vocab_size=vocab_size\n",
    "        bert.config.num_attention_heads=num_heads\n",
    "        bert.config.num_hidden_layers=num_layers\n",
    "        bert.config.pad_token_id=27\n",
    "        bert.config.intermediate_size=latent_dim\n",
    "        bert.config.max_position_embeddings=max_seq_len\n",
    "        self.bert_model = BertModel(bert.config)\n",
    "        self.classifier = torch.nn.Linear(model_dim, 26)\n",
    "        self.config = bert.config\n",
    "        self.guessed_letters = []\n",
    "        self.alphabet = np.array([chr(97+i) for i in range(26)])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def flush(self):\n",
    "        \"\"\" Resets the Hangman game.\n",
    "        \"\"\"\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "    def guess(self, pattern):\n",
    "        if len(pattern)>self.tokenizer.max_seq_len:\n",
    "            \"\"\" Less than 0.1% of all training words have\n",
    "                length larger than 20, so we \"ignore\" those.\n",
    "            \"\"\"\n",
    "            letters_argsort = range(26)\n",
    "        else:\n",
    "            inp = self.tokenizer(pattern)\n",
    "            inp['input_ids'] = inp['input_ids'].unsqueeze(0)\n",
    "            inp['attention_mask'] = inp['attention_mask'].unsqueeze(0)\n",
    "            out = F.softmax(self(inp), dim=-1).squeeze()\n",
    "            letters_argsort = sorted(range(26), key=lambda i: out[i], reverse=True)\n",
    "        letters = self.alphabet[letters_argsort]\n",
    "        for letter in letters:\n",
    "            if letter not in self.guessed_letters:\n",
    "                self.guessed_letters.append(letter)\n",
    "                return letter\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        attention_mask = inp['attention_mask'][:, None, None, :].to(self.device)\n",
    "        input_ids = inp['input_ids'].to(self.device)\n",
    "        x = self.bert_model.embeddings(input_ids)\n",
    "        x = self.bert_model.encoder(x, attention_mask=attention_mask)\n",
    "        x = mean_pooling(x['last_hidden_state'], inp['attention_mask'])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanTokenizer():\n",
    "    def __init__(self, max_seq_len, device):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.special_idxs = {\n",
    "            'pad_idx': 27,\n",
    "            'dot_idx': 26}\n",
    "        self.device=device\n",
    "        \n",
    "    def __call__(self, pattern, word=None):\n",
    "        \"\"\" Tokenizes a given pattern (word).\n",
    "        \"\"\"\n",
    "        inp = torch.zeros(self.max_seq_len)\n",
    "        attn_mask = torch.zeros(self.max_seq_len)\n",
    "        fill_mask = torch.zeros(26)\n",
    "        label = torch.zeros(26)\n",
    "        for j in range(len(pattern)):\n",
    "            if pattern[j].isalpha():\n",
    "                inp[j] = ord(pattern[j])-97\n",
    "                fill_mask[ord(pattern[j])-97] = -1e9\n",
    "            else:\n",
    "                inp[j] = self.special_idxs['dot_idx']\n",
    "                if word is not None:\n",
    "                    label[ord(word[j])-97] = 1\n",
    "                    y = torch.LongTensor([ord(word[j])-97])\n",
    "                else:\n",
    "                    y = torch.LongTensor([0])\n",
    "            attn_mask[j] = 1\n",
    "        for j in range(len(pattern), self.max_seq_len):\n",
    "            inp[j] = self.special_idxs['pad_idx']\n",
    "        out = {'input_ids': inp.long().to(self.device),\n",
    "               'attention_mask': attn_mask.long().to(self.device),\n",
    "               'fill_mask': fill_mask.float().to(self.device),\n",
    "               'label': label.float().to(self.device),\n",
    "               'y': y.to(self.device)}\n",
    "        return out\n",
    "        \n",
    "\n",
    "class HangmanDatasetStage1(Dataset):\n",
    "    \"\"\" This dataset realizes masking of a\n",
    "        single letter in a word (for Stage 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, tokenizer):\n",
    "        self.dictionary = []\n",
    "        for word in dictionary:\n",
    "            if len(word) <= tokenizer.max_seq_len:\n",
    "                self.dictionary.append(word)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        word = self.dictionary[i]\n",
    "        idx = np.random.choice(range(len(word)))\n",
    "        pattern = \"\"\n",
    "        for i,letter in enumerate(word):\n",
    "            if i!=idx:\n",
    "                pattern += letter\n",
    "            else:\n",
    "                pattern += '.'\n",
    "        return self.tokenizer(pattern, word=word)\n",
    "\n",
    "\n",
    "class HangmanDatasetStage2(Dataset):\n",
    "    \"\"\" This dataset realizes masking of all instances of\n",
    "        letters in a random subcollection of\n",
    "        letters in a word (for Stage 2).\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, tokenizer):\n",
    "        self.dictionary = []\n",
    "        for word in dictionary:\n",
    "            if len(word) <= tokenizer.max_seq_len:\n",
    "                self.dictionary.append(word)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        word = self.dictionary[i]\n",
    "        word_letters = set(list(word))\n",
    "        if len(word_letters)>1:\n",
    "            num_open = np.random.choice(range(1,len(word_letters)))\n",
    "            open_letters = np.random.choice(\n",
    "                list(word_letters),\n",
    "                size=num_open,\n",
    "                replace=False)\n",
    "            open_letters = set(open_letters.tolist())\n",
    "        else:\n",
    "            open_letters = set([])\n",
    "        pattern = \"\"\n",
    "        for letter in word:\n",
    "            if letter in open_letters:\n",
    "                pattern += letter\n",
    "            else:\n",
    "                pattern += '.'\n",
    "        return self.tokenizer(pattern, word=word)\n",
    "        \n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {'input_ids': torch.vstack([inp['input_ids'] for inp in batch]),\n",
    "        'attention_mask': torch.vstack([inp['attention_mask'] for inp in batch]),\n",
    "        'fill_mask': torch.vstack([inp['fill_mask'] for inp in batch]),\n",
    "        'label': torch.vstack([inp['label'] for inp in batch]),\n",
    "        'y': torch.cat([inp['y'] for inp in batch])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary, test_dictionary = build_dictionaries()\n",
    "tokenizer = HangmanTokenizer(max_seq_len=20, device=device)\n",
    "dataset = HangmanDatasetStage1(train_dictionary, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "model = HangmanTransformer(\n",
    "    max_seq_len=20,\n",
    "    model_dim=512,\n",
    "    latent_dim=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_heads=8,\n",
    "    num_layers=12,\n",
    "    vocab_size=26+len(tokenizer.special_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Pre-training\n",
    "In this stage, we pre-train ```HangmanTransformer``` using \"masked word modeling\". We use a learning rate linear warmup for 10 epochs followed by exponential decay. Gamma is computed such that the final learning rate is 50 times smaller than the maximum one (reached at the end of warmup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0][loss/acc: 2.7703/0.1515][lr: 0.00002]\n",
      "[epoch: 1][loss/acc: 2.5372/0.2112][lr: 0.00004]\n",
      "[epoch: 2][loss/acc: 2.1394/0.3147][lr: 0.00006]\n",
      "[epoch: 3][loss/acc: 1.9439/0.3678][lr: 0.00007]\n",
      "[epoch: 4][loss/acc: 1.8411/0.3988][lr: 0.00009]\n",
      "[epoch: 5][loss/acc: 1.7687/0.4224][lr: 0.00011]\n",
      "[epoch: 6][loss/acc: 1.7089/0.4420][lr: 0.00013]\n",
      "[epoch: 7][loss/acc: 1.6662/0.4560][lr: 0.00015]\n",
      "[epoch: 8][loss/acc: 1.6250/0.4692][lr: 0.00016]\n",
      "[epoch: 9][loss/acc: 1.5897/0.4804][lr: 0.00018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10][loss/acc: 1.5631/0.4893][lr: 0.00020]\n",
      "[epoch: 11][loss/acc: 1.5200/0.5028][lr: 0.00019]\n",
      "[epoch: 12][loss/acc: 1.4760/0.5167][lr: 0.00018]\n",
      "[epoch: 13][loss/acc: 1.4392/0.5287][lr: 0.00017]\n",
      "[epoch: 14][loss/acc: 1.4043/0.5396][lr: 0.00016]\n",
      "[epoch: 15][loss/acc: 1.3779/0.5496][lr: 0.00015]\n",
      "[epoch: 16][loss/acc: 1.3494/0.5572][lr: 0.00015]\n",
      "[epoch: 17][loss/acc: 1.3220/0.5665][lr: 0.00014]\n",
      "[epoch: 18][loss/acc: 1.2979/0.5751][lr: 0.00013]\n",
      "[epoch: 19][loss/acc: 1.2754/0.5818][lr: 0.00013]\n",
      "[epoch: 20][loss/acc: 1.2505/0.5903][lr: 0.00012]\n",
      "[epoch: 21][loss/acc: 1.2280/0.5970][lr: 0.00011]\n",
      "[epoch: 22][loss/acc: 1.2083/0.6036][lr: 0.00011]\n",
      "[epoch: 23][loss/acc: 1.1871/0.6107][lr: 0.00010]\n",
      "[epoch: 24][loss/acc: 1.1705/0.6169][lr: 0.00010]\n",
      "[epoch: 25][loss/acc: 1.1582/0.6208][lr: 0.00009]\n",
      "[epoch: 26][loss/acc: 1.1375/0.6271][lr: 0.00009]\n",
      "[epoch: 27][loss/acc: 1.1179/0.6323][lr: 0.00008]\n",
      "[epoch: 28][loss/acc: 1.1092/0.6359][lr: 0.00008]\n",
      "[epoch: 29][loss/acc: 1.0948/0.6408][lr: 0.00007]\n",
      "[epoch: 30][loss/acc: 1.0816/0.6445][lr: 0.00007]\n",
      "[epoch: 31][loss/acc: 1.0678/0.6485][lr: 0.00007]\n",
      "[epoch: 32][loss/acc: 1.0538/0.6533][lr: 0.00006]\n",
      "[epoch: 33][loss/acc: 1.0466/0.6556][lr: 0.00006]\n",
      "[epoch: 34][loss/acc: 1.0374/0.6593][lr: 0.00006]\n",
      "[epoch: 35][loss/acc: 1.0236/0.6626][lr: 0.00005]\n",
      "[epoch: 36][loss/acc: 1.0160/0.6666][lr: 0.00005]\n",
      "[epoch: 37][loss/acc: 1.0051/0.6703][lr: 0.00005]\n",
      "[epoch: 38][loss/acc: 0.9950/0.6714][lr: 0.00005]\n",
      "[epoch: 39][loss/acc: 0.9858/0.6756][lr: 0.00004]\n",
      "[epoch: 40][loss/acc: 0.9777/0.6780][lr: 0.00004]\n",
      "[epoch: 41][loss/acc: 0.9721/0.6788][lr: 0.00004]\n",
      "[epoch: 42][loss/acc: 0.9631/0.6836][lr: 0.00004]\n",
      "[epoch: 43][loss/acc: 0.9518/0.6871][lr: 0.00004]\n",
      "[epoch: 44][loss/acc: 0.9474/0.6877][lr: 0.00003]\n",
      "[epoch: 45][loss/acc: 0.9457/0.6882][lr: 0.00003]\n",
      "[epoch: 46][loss/acc: 0.9409/0.6894][lr: 0.00003]\n",
      "[epoch: 47][loss/acc: 0.9331/0.6924][lr: 0.00003]\n",
      "[epoch: 48][loss/acc: 0.9252/0.6943][lr: 0.00003]\n",
      "[epoch: 49][loss/acc: 0.9230/0.6955][lr: 0.00003]\n",
      "[epoch: 50][loss/acc: 0.9136/0.6985][lr: 0.00002]\n",
      "[epoch: 51][loss/acc: 0.9093/0.6990][lr: 0.00002]\n",
      "[epoch: 52][loss/acc: 0.9041/0.7003][lr: 0.00002]\n",
      "[epoch: 53][loss/acc: 0.9017/0.7022][lr: 0.00002]\n",
      "[epoch: 54][loss/acc: 0.8978/0.7039][lr: 0.00002]\n",
      "[epoch: 55][loss/acc: 0.8943/0.7051][lr: 0.00002]\n",
      "[epoch: 56][loss/acc: 0.8900/0.7060][lr: 0.00002]\n",
      "[epoch: 57][loss/acc: 0.8862/0.7069][lr: 0.00002]\n",
      "[epoch: 58][loss/acc: 0.8850/0.7070][lr: 0.00002]\n",
      "[epoch: 59][loss/acc: 0.8760/0.7104][lr: 0.00002]\n",
      "[epoch: 60][loss/acc: 0.8731/0.7107][lr: 0.00001]\n",
      "[epoch: 61][loss/acc: 0.8733/0.7110][lr: 0.00001]\n",
      "[epoch: 62][loss/acc: 0.8691/0.7123][lr: 0.00001]\n",
      "[epoch: 63][loss/acc: 0.8666/0.7126][lr: 0.00001]\n",
      "[epoch: 64][loss/acc: 0.8648/0.7135][lr: 0.00001]\n",
      "[epoch: 65][loss/acc: 0.8622/0.7148][lr: 0.00001]\n",
      "[epoch: 66][loss/acc: 0.8589/0.7158][lr: 0.00001]\n",
      "[epoch: 67][loss/acc: 0.8579/0.7173][lr: 0.00001]\n",
      "[epoch: 68][loss/acc: 0.8533/0.7173][lr: 0.00001]\n",
      "[epoch: 69][loss/acc: 0.8525/0.7165][lr: 0.00001]\n",
      "[epoch: 70][loss/acc: 0.8530/0.7179][lr: 0.00001]\n",
      "[epoch: 71][loss/acc: 0.8482/0.7197][lr: 0.00001]\n",
      "[epoch: 72][loss/acc: 0.8489/0.7179][lr: 0.00001]\n",
      "[epoch: 73][loss/acc: 0.8470/0.7189][lr: 0.00001]\n",
      "[epoch: 74][loss/acc: 0.8466/0.7184][lr: 0.00001]\n"
     ]
    }
   ],
   "source": [
    "gamma = (1./50)**(1./75)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=2e-4,\n",
    "    weight_decay=1e-5)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=10)\n",
    "scheduler_decay = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=gamma)\n",
    "schedulers = [scheduler_warmup, scheduler_decay]\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers,\n",
    "    milestones=[10])\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(75):\n",
    "    losses = []\n",
    "    acc = 0.\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y = batch['y']\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        acc += sum(torch.argmax(out, dim=-1) == y)\n",
    "    acc = acc/len(dataset)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'[epoch: {epoch}][loss/acc: {np.mean(losses):.4f}/{acc:.4f}][lr: {lr:.5f}]')\n",
    "    scheduler.step()\n",
    "    \n",
    "state_dict = model.state_dict()\n",
    "#torch.save(state_dict, 'model-checkpoint-stage-1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Finetuning\n",
    "Now that we pre-trained a model to recognize word structures and grammar, we finetune it on a more realistic data distribution (with more than one blanks in words) as generated by ```HangmanDatasetStage2```. Now, the objective function is to guess all of the letters that are masked out in the input, so we use a multi-label BCE loss. In addition, we utilize the fact that none of the revealed letters can be masked and effectively \"mask\" them out in the output by using ```fill_mask``` returned by the tokenizer. To pick the best model during training, we perform validation using a hold-out dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dictionary, verbose=False, n=100):\n",
    "    \"\"\" Hangman game simulator\n",
    "    \"\"\"\n",
    "    successes = 0.\n",
    "    words = np.random.choice(dictionary, size=n, replace=False)\n",
    "    for word in tqdm(words):\n",
    "        model.flush()\n",
    "        pattern = ['.']*len(word)\n",
    "        if verbose:\n",
    "            print(f'[GAME]: {\"\".join(pattern)}')\n",
    "        missed = 0\n",
    "        while True:\n",
    "            letter = model.guess(\"\".join(pattern))\n",
    "            guess_correct = False\n",
    "            for i in range(len(word)):\n",
    "                if word[i] == letter:\n",
    "                    pattern[i] = letter\n",
    "                    guess_correct = True\n",
    "            if not guess_correct:\n",
    "                missed += 1\n",
    "            if missed == 6:\n",
    "                if verbose:\n",
    "                    print(f'[FAIL][word: {word}]')\n",
    "                break\n",
    "            elif '.' not in pattern:\n",
    "                successes += 1\n",
    "                if verbose:\n",
    "                    print(f'[OK!][word: {word}]')\n",
    "                break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'-[{6-missed}]: {\"\".join(pattern)} ({letter})')\n",
    "    return successes/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0][loss/acc: 0.3371/0.3100]\n",
      "[epoch: 1][loss/acc: 0.2823/0.3100]\n",
      "[epoch: 2][loss/acc: 0.2726/0.3100]\n",
      "[epoch: 3][loss/acc: 0.2676/0.3100]\n",
      "[epoch: 4][loss/acc: 0.2651/0.3100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5][loss/acc: 0.2617/0.6000]\n",
      "[epoch: 6][loss/acc: 0.2598/0.6000]\n",
      "[epoch: 7][loss/acc: 0.2583/0.6000]\n",
      "[epoch: 8][loss/acc: 0.2574/0.6000]\n",
      "[epoch: 9][loss/acc: 0.2554/0.6000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10][loss/acc: 0.2546/0.5500]\n",
      "[epoch: 11][loss/acc: 0.2532/0.5500]\n",
      "[epoch: 12][loss/acc: 0.2525/0.5500]\n",
      "[epoch: 13][loss/acc: 0.2513/0.5500]\n",
      "[epoch: 14][loss/acc: 0.2513/0.5500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 15][loss/acc: 0.2506/0.5200]\n",
      "[epoch: 16][loss/acc: 0.2490/0.5200]\n",
      "[epoch: 17][loss/acc: 0.2493/0.5200]\n",
      "[epoch: 18][loss/acc: 0.2487/0.5200]\n",
      "[epoch: 19][loss/acc: 0.2485/0.5200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 20][loss/acc: 0.2482/0.6200]\n",
      "[epoch: 21][loss/acc: 0.2480/0.6200]\n",
      "[epoch: 22][loss/acc: 0.2478/0.6200]\n",
      "[epoch: 23][loss/acc: 0.2473/0.6200]\n",
      "[epoch: 24][loss/acc: 0.2467/0.6200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 25][loss/acc: 0.2474/0.6200]\n",
      "[epoch: 26][loss/acc: 0.2470/0.6200]\n",
      "[epoch: 27][loss/acc: 0.2471/0.6200]\n",
      "[epoch: 28][loss/acc: 0.2465/0.6200]\n",
      "[epoch: 29][loss/acc: 0.2465/0.6200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 30][loss/acc: 0.2458/0.6100]\n",
      "[epoch: 31][loss/acc: 0.2462/0.6100]\n",
      "[epoch: 32][loss/acc: 0.2457/0.6100]\n",
      "[epoch: 33][loss/acc: 0.2460/0.6100]\n",
      "[epoch: 34][loss/acc: 0.2458/0.6100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 35][loss/acc: 0.2459/0.5800]\n",
      "[epoch: 36][loss/acc: 0.2459/0.5800]\n",
      "[epoch: 37][loss/acc: 0.2463/0.5800]\n",
      "[epoch: 38][loss/acc: 0.2461/0.5800]\n",
      "[epoch: 39][loss/acc: 0.2455/0.5800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 40][loss/acc: 0.2445/0.6900]\n",
      "[epoch: 41][loss/acc: 0.2455/0.6900]\n",
      "[epoch: 42][loss/acc: 0.2455/0.6900]\n",
      "[epoch: 43][loss/acc: 0.2452/0.6900]\n",
      "[epoch: 44][loss/acc: 0.2451/0.6900]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 45][loss/acc: 0.2454/0.5600]\n",
      "[epoch: 46][loss/acc: 0.2456/0.5600]\n",
      "[epoch: 47][loss/acc: 0.2451/0.5600]\n",
      "[epoch: 48][loss/acc: 0.2449/0.5600]\n",
      "[epoch: 49][loss/acc: 0.2453/0.5600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:25<00:00,  7.83it/s]\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('model-checkpoint-stage-1.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "gamma = (1./50)**(1./50)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.bert_model.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}],\n",
    "    lr=1e-4)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=10)\n",
    "scheduler_decay = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=gamma)\n",
    "schedulers = [scheduler_warmup, scheduler_decay]\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers,\n",
    "    milestones=[10])\n",
    "dataset = HangmanDatasetStage2(train_dictionary, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "best_model = {\n",
    "    'accuracy': 0,\n",
    "    'model': None}\n",
    "for epoch in range(50):\n",
    "    losses = []\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        acc = validate(model, test_dictionary, n=100)\n",
    "        if best_model['accuracy'] < acc:\n",
    "            best_model['model'] = model.state_dict()\n",
    "        torch.save(best_model['model'], 'model-checkpoint-stage-2.pt')\n",
    "        model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        out = out + batch['fill_mask']\n",
    "        loss = criterion(out, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    scheduler.step()\n",
    "    print(f'[epoch: {epoch}][loss/acc: {np.mean(losses):.4f}/{acc:.4f}]')\n",
    "    \n",
    "model.eval()\n",
    "acc = validate(model, test_dictionary, n=200)\n",
    "if best_model['accuracy'] < acc:\n",
    "    best_model['model'] = model.state_dict()\n",
    "#torch.save(best_model['model'], 'model-checkpoint-stage-2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbose Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model ready]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = HangmanTokenizer(max_seq_len=20, device=device)\n",
    "final_model = HangmanTransformer(\n",
    "    max_seq_len=20,\n",
    "    model_dim=512,\n",
    "    latent_dim=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_heads=8,\n",
    "    num_layers=12,\n",
    "    vocab_size=26+len(tokenizer.special_idxs))\n",
    "state_dict = torch.load('model-checkpoint-stage-2.pt')\n",
    "final_model.load_state_dict(state_dict)\n",
    "final_model.eval()\n",
    "print(f'[Model ready]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:03<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[accuracy: 0.624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple eval.\n",
    "\n",
    "N = 500\n",
    "word = np.random.choice(test_dictionary, size=N, replace=False)\n",
    "acc = validate(final_model, word, verbose=False, n=N)\n",
    "print(f'[accuracy: {acc:.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAME]: .......\n",
      "-[5]: ....... (e)\n",
      "-[4]: ....... (a)\n",
      "-[4]: .....r. (r)\n",
      "-[4]: ...s.r. (s)\n",
      "-[4]: .o.sor. (o)\n",
      "-[4]: .onsor. (n)\n",
      "-[4]: consor. (c)\n",
      "[OK!][word: consort]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Single eval.\n",
    "\n",
    "word = np.random.choice(test_dictionary)\n",
    "acc = validate(final_model, [word], verbose=True, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
