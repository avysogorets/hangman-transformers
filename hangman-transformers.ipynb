{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hangman\n",
    "In Hangman, you need to guess a word where all of its letters are hidden behind masks. In each turn, you name a letter and, if it appears in the word, all instances of it are unmasked. If it does not, you incur a penalty loss. You lose the game if your penalty reaches 6 before you guess the word and win otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "The key to playing Hangman, to a large extent, lies in extracting patterns of natural language such as word structure and grammar. Thus, we adopt the NLP framework and attempt to solve the task with a BERT-like transformer. The vocabulary will consist of 26 English letters plus two special tokens: mask and padding; the output layer is a classification head with 26 outputs. The transformer acrhitecture is suitabe for the task as it can naturally learn to focus on some local patterns within words via the attention mechanism. In fact, it is common to train LLMs using \"masked language modeling\", which is similar to the given problem. The proposed procedure is two-stage:\n",
    "- ***Stage 1:*** we first pre-train a transformer and teach it to identify patterns in words. To this end, akin to masked language modeling, we mask *one* random letter in each word with the goal of predicting it.\n",
    "- ***Stage 2:*** since the model will often be presented with more than one blanks in one word, we need to finetune it on the distribution of inputs as close to the real game scenario as possible. To this end, for each training word, we randomly mask out all instances of some subcollection of its letters and perform a multi-label classification with the goal of predicting all of those masked letters.\n",
    "\n",
    "Please see additional comments below for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/amv458/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device cuda:0 is ready]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'[device {device} is ready]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionaries(train_file=\"words_250000_train.txt\"):\n",
    "    text_file = open(train_file, \"r\")\n",
    "    train_dictionary = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    train_set = set(train_dictionary)\n",
    "    test_dictionary = []\n",
    "    for synset in wordnet.all_synsets():\n",
    "        for lemma in synset.lemmas():\n",
    "            word = lemma.name().lower()\n",
    "            if word not in train_set and word.isalpha() :\n",
    "                test_dictionary.append(word)\n",
    "    return train_dictionary, test_dictionary\n",
    "\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\" Average pooling layer for the Transformer.\n",
    "    \"\"\"\n",
    "    token_embeddings = token_embeddings.to(attention_mask.device)\n",
    "    input_mask_expanded=(attention_mask.unsqueeze(-1).float())\n",
    "    sum_embeddings=torch.sum(token_embeddings * input_mask_expanded, -2)\n",
    "    sum_mask=torch.clamp(input_mask_expanded.sum(-2), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class HangmanTransformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "            max_seq_len,\n",
    "            model_dim,\n",
    "            latent_dim,\n",
    "            tokenizer,\n",
    "            device=torch.device('cpu'),\n",
    "            num_heads=1,\n",
    "            num_layers=1,\n",
    "            vocab_size=28):\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        bert.config.hidden_size=model_dim\n",
    "        bert.config.vocab_size=vocab_size\n",
    "        bert.config.num_attention_heads=num_heads\n",
    "        bert.config.num_hidden_layers=num_layers\n",
    "        bert.config.pad_token_id=27\n",
    "        bert.config.intermediate_size=latent_dim\n",
    "        bert.config.max_position_embeddings=max_seq_len\n",
    "        self.bert_model = BertModel(bert.config)\n",
    "        self.classifier = torch.nn.Linear(model_dim, 26)\n",
    "        self.config = bert.config\n",
    "        self.guessed_letters = []\n",
    "        self.alphabet = np.array([chr(97+i) for i in range(26)])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def flush(self):\n",
    "        \"\"\" Resets the Hangman game.\n",
    "        \"\"\"\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "    def guess(self, pattern):\n",
    "        if len(pattern)>self.tokenizer.max_seq_len:\n",
    "            \"\"\" Less than 0.1% of all training words have\n",
    "                length larger than 20, so we \"ignore\" those.\n",
    "            \"\"\"\n",
    "            letters_argsort = range(26)\n",
    "        else:\n",
    "            inp = self.tokenizer(pattern)\n",
    "            inp['input_ids'] = inp['input_ids'].unsqueeze(0)\n",
    "            inp['attention_mask'] = inp['attention_mask'].unsqueeze(0)\n",
    "            out = F.softmax(self(inp), dim=-1).squeeze()\n",
    "            letters_argsort = sorted(range(26), key=lambda i: out[i], reverse=True)\n",
    "        letters = self.alphabet[letters_argsort]\n",
    "        for letter in letters:\n",
    "            if letter not in self.guessed_letters:\n",
    "                self.guessed_letters.append(letter)\n",
    "                return letter\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        attention_mask = inp['attention_mask'][:, None, None, :].to(self.device)\n",
    "        input_ids = inp['input_ids'].to(self.device)\n",
    "        x = self.bert_model.embeddings(input_ids)\n",
    "        x = self.bert_model.encoder(x, attention_mask=attention_mask)\n",
    "        x = mean_pooling(x['last_hidden_state'], inp['attention_mask'])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanTokenizer():\n",
    "    def __init__(self, max_seq_len, device):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.special_idxs = {\n",
    "            'pad_idx': 27,\n",
    "            'dot_idx': 26}\n",
    "        self.device=device\n",
    "        \n",
    "    def __call__(self, pattern, word=None):\n",
    "        \"\"\" Tokenizes a given pattern (word).\n",
    "        \"\"\"\n",
    "        inp = torch.zeros(self.max_seq_len)\n",
    "        attn_mask = torch.zeros(self.max_seq_len)\n",
    "        fill_mask = torch.zeros(26)\n",
    "        label = torch.zeros(26)\n",
    "        for j in range(len(pattern)):\n",
    "            if pattern[j].isalpha():\n",
    "                inp[j] = ord(pattern[j])-97\n",
    "                fill_mask[ord(pattern[j])-97] = -1e9\n",
    "            else:\n",
    "                inp[j] = self.special_idxs['dot_idx']\n",
    "                if word is not None:\n",
    "                    label[ord(word[j])-97] = 1\n",
    "                    y = torch.LongTensor([ord(word[j])-97])\n",
    "                else:\n",
    "                    y = torch.LongTensor([0])\n",
    "            attn_mask[j] = 1\n",
    "        for j in range(len(pattern), self.max_seq_len):\n",
    "            inp[j] = self.special_idxs['pad_idx']\n",
    "        out = {'input_ids': inp.long().to(self.device),\n",
    "               'attention_mask': attn_mask.long().to(self.device),\n",
    "               'fill_mask': fill_mask.float().to(self.device),\n",
    "               'label': label.float().to(self.device),\n",
    "               'y': y.to(self.device)}\n",
    "        return out\n",
    "        \n",
    "\n",
    "class HangmanDatasetStage1(Dataset):\n",
    "    \"\"\" This dataset realizes masking of a\n",
    "        single letter in a word (for Stage 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, tokenizer):\n",
    "        self.dictionary = []\n",
    "        for word in dictionary:\n",
    "            if len(word) <= tokenizer.max_seq_len:\n",
    "                self.dictionary.append(word)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        word = self.dictionary[i]\n",
    "        idx = np.random.choice(range(len(word)))\n",
    "        pattern = \"\"\n",
    "        for i,letter in enumerate(word):\n",
    "            if i!=idx:\n",
    "                pattern += letter\n",
    "            else:\n",
    "                pattern += '.'\n",
    "        return self.tokenizer(pattern, word=word)\n",
    "\n",
    "\n",
    "class HangmanDatasetStage2(Dataset):\n",
    "    \"\"\" This dataset realizes masking of all instances of\n",
    "        letters in a random subcollection of\n",
    "        letters in a word (for Stage 2).\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary, tokenizer):\n",
    "        self.dictionary = []\n",
    "        for word in dictionary:\n",
    "            if len(word) <= tokenizer.max_seq_len:\n",
    "                self.dictionary.append(word)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        word = self.dictionary[i]\n",
    "        word_letters = set(list(word))\n",
    "        if len(word_letters)>1:\n",
    "            num_open = np.random.choice(range(1,len(word_letters)))\n",
    "            open_letters = np.random.choice(\n",
    "                list(word_letters),\n",
    "                size=num_open,\n",
    "                replace=False)\n",
    "            open_letters = set(open_letters.tolist())\n",
    "        else:\n",
    "            open_letters = set([])\n",
    "        pattern = \"\"\n",
    "        for letter in word:\n",
    "            if letter in open_letters:\n",
    "                pattern += letter\n",
    "            else:\n",
    "                pattern += '.'\n",
    "        return self.tokenizer(pattern, word=word)\n",
    "        \n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {'input_ids': torch.vstack([inp['input_ids'] for inp in batch]),\n",
    "        'attention_mask': torch.vstack([inp['attention_mask'] for inp in batch]),\n",
    "        'fill_mask': torch.vstack([inp['fill_mask'] for inp in batch]),\n",
    "        'label': torch.vstack([inp['label'] for inp in batch]),\n",
    "        'y': torch.cat([inp['y'] for inp in batch])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Pre-training\n",
    "In this stage, we pre-train ```HangmanTransformer``` using \"masked word modeling\". We use a learning rate linear warmup for 10 epochs followed by exponential decay. Gamma is computed such that the final learning rate is 50 times smaller than the maximum one (reached at the end of warmup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0][loss/acc: 2.7580/0.1544]\n",
      "[epoch: 1][loss/acc: 2.6367/0.2177]\n",
      "[epoch: 2][loss/acc: 2.4676/0.3169]\n",
      "[epoch: 3][loss/acc: 2.3372/0.3681]\n",
      "[epoch: 4][loss/acc: 2.2381/0.3992]\n",
      "[epoch: 5][loss/acc: 2.1600/0.4210]\n",
      "[epoch: 6][loss/acc: 2.0960/0.4416]\n",
      "[epoch: 7][loss/acc: 2.0418/0.4561]\n",
      "[epoch: 8][loss/acc: 1.9953/0.4698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 9][loss/acc: 1.9547/0.4803]\n",
      "[epoch: 10][loss/acc: 1.9186/0.4916]\n",
      "[epoch: 11][loss/acc: 1.8847/0.5054]\n",
      "[epoch: 12][loss/acc: 1.8532/0.5189]\n",
      "[epoch: 13][loss/acc: 1.8237/0.5290]\n",
      "[epoch: 14][loss/acc: 1.7957/0.5404]\n",
      "[epoch: 15][loss/acc: 1.7696/0.5494]\n",
      "[epoch: 16][loss/acc: 1.7446/0.5607]\n",
      "[epoch: 17][loss/acc: 1.7210/0.5677]\n",
      "[epoch: 18][loss/acc: 1.6988/0.5756]\n",
      "[epoch: 19][loss/acc: 1.6775/0.5835]\n",
      "[epoch: 20][loss/acc: 1.6573/0.5908]\n",
      "[epoch: 21][loss/acc: 1.6379/0.5961]\n",
      "[epoch: 22][loss/acc: 1.6193/0.6039]\n",
      "[epoch: 23][loss/acc: 1.6013/0.6104]\n",
      "[epoch: 24][loss/acc: 1.5841/0.6165]\n",
      "[epoch: 25][loss/acc: 1.5675/0.6218]\n",
      "[epoch: 26][loss/acc: 1.5517/0.6264]\n",
      "[epoch: 27][loss/acc: 1.5364/0.6319]\n",
      "[epoch: 28][loss/acc: 1.5216/0.6360]\n",
      "[epoch: 29][loss/acc: 1.5075/0.6401]\n",
      "[epoch: 30][loss/acc: 1.4938/0.6460]\n",
      "[epoch: 31][loss/acc: 1.4805/0.6489]\n",
      "[epoch: 32][loss/acc: 1.4677/0.6530]\n",
      "[epoch: 33][loss/acc: 1.4552/0.6568]\n",
      "[epoch: 34][loss/acc: 1.4431/0.6615]\n",
      "[epoch: 35][loss/acc: 1.4314/0.6643]\n",
      "[epoch: 36][loss/acc: 1.4200/0.6677]\n",
      "[epoch: 37][loss/acc: 1.4090/0.6700]\n",
      "[epoch: 38][loss/acc: 1.3983/0.6744]\n",
      "[epoch: 39][loss/acc: 1.3879/0.6768]\n",
      "[epoch: 40][loss/acc: 1.3777/0.6804]\n",
      "[epoch: 41][loss/acc: 1.3678/0.6838]\n",
      "[epoch: 42][loss/acc: 1.3582/0.6850]\n",
      "[epoch: 43][loss/acc: 1.3488/0.6883]\n",
      "[epoch: 44][loss/acc: 1.3397/0.6908]\n",
      "[epoch: 45][loss/acc: 1.3307/0.6946]\n",
      "[epoch: 46][loss/acc: 1.3221/0.6957]\n",
      "[epoch: 47][loss/acc: 1.3136/0.6979]\n",
      "[epoch: 48][loss/acc: 1.3054/0.7007]\n",
      "[epoch: 49][loss/acc: 1.2974/0.7023]\n",
      "[epoch: 50][loss/acc: 1.2895/0.7032]\n",
      "[epoch: 51][loss/acc: 1.2818/0.7072]\n",
      "[epoch: 52][loss/acc: 1.2743/0.7083]\n",
      "[epoch: 53][loss/acc: 1.2669/0.7097]\n",
      "[epoch: 54][loss/acc: 1.2597/0.7127]\n",
      "[epoch: 55][loss/acc: 1.2526/0.7132]\n",
      "[epoch: 56][loss/acc: 1.2457/0.7155]\n",
      "[epoch: 57][loss/acc: 1.2390/0.7160]\n",
      "[epoch: 58][loss/acc: 1.2324/0.7180]\n",
      "[epoch: 59][loss/acc: 1.2259/0.7205]\n",
      "[epoch: 60][loss/acc: 1.2196/0.7215]\n",
      "[epoch: 61][loss/acc: 1.2135/0.7227]\n",
      "[epoch: 62][loss/acc: 1.2073/0.7244]\n",
      "[epoch: 63][loss/acc: 1.2014/0.7254]\n",
      "[epoch: 64][loss/acc: 1.1955/0.7271]\n",
      "[epoch: 65][loss/acc: 1.1899/0.7273]\n",
      "[epoch: 66][loss/acc: 1.1843/0.7281]\n",
      "[epoch: 67][loss/acc: 1.1787/0.7316]\n",
      "[epoch: 68][loss/acc: 1.1734/0.7305]\n",
      "[epoch: 69][loss/acc: 1.1681/0.7324]\n",
      "[epoch: 70][loss/acc: 1.1630/0.7327]\n",
      "[epoch: 71][loss/acc: 1.1580/0.7338]\n",
      "[epoch: 72][loss/acc: 1.1530/0.7339]\n",
      "[epoch: 73][loss/acc: 1.1481/0.7367]\n",
      "[epoch: 74][loss/acc: 1.1433/0.7367]\n",
      "[epoch: 75][loss/acc: 1.1386/0.7377]\n",
      "[epoch: 76][loss/acc: 1.1340/0.7376]\n",
      "[epoch: 77][loss/acc: 1.1295/0.7386]\n",
      "[epoch: 78][loss/acc: 1.1251/0.7400]\n",
      "[epoch: 79][loss/acc: 1.1207/0.7405]\n",
      "[epoch: 80][loss/acc: 1.1165/0.7425]\n",
      "[epoch: 81][loss/acc: 1.1123/0.7424]\n",
      "[epoch: 82][loss/acc: 1.1081/0.7427]\n",
      "[epoch: 83][loss/acc: 1.1041/0.7446]\n",
      "[epoch: 84][loss/acc: 1.1001/0.7431]\n",
      "[epoch: 85][loss/acc: 1.0963/0.7437]\n",
      "[epoch: 86][loss/acc: 1.0924/0.7462]\n",
      "[epoch: 87][loss/acc: 1.0886/0.7465]\n",
      "[epoch: 88][loss/acc: 1.0849/0.7468]\n",
      "[epoch: 89][loss/acc: 1.0812/0.7483]\n",
      "[epoch: 90][loss/acc: 1.0776/0.7482]\n",
      "[epoch: 91][loss/acc: 1.0741/0.7483]\n",
      "[epoch: 92][loss/acc: 1.0706/0.7484]\n",
      "[epoch: 93][loss/acc: 1.0673/0.7485]\n",
      "[epoch: 94][loss/acc: 1.0639/0.7502]\n",
      "[epoch: 95][loss/acc: 1.0606/0.7506]\n",
      "[epoch: 96][loss/acc: 1.0573/0.7489]\n",
      "[epoch: 97][loss/acc: 1.0541/0.7505]\n",
      "[epoch: 98][loss/acc: 1.0510/0.7499]\n",
      "[epoch: 99][loss/acc: 1.0479/0.7516]\n"
     ]
    }
   ],
   "source": [
    "train_dictionary, test_dictionary = build_dictionaries()\n",
    "tokenizer = HangmanTokenizer(max_seq_len=20, device=device)\n",
    "dataset = HangmanDatasetStage1(train_dictionary, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "model = HangmanTransformer(\n",
    "    max_seq_len=20,\n",
    "    model_dim=512,\n",
    "    latent_dim=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_heads=8,\n",
    "    num_layers=12,\n",
    "    vocab_size=26+len(tokenizer.special_idxs))\n",
    "gamma = (1./50)**(1./100)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=2e-4,\n",
    "    weight_decay=1e-5)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=10)\n",
    "scheduler_decay = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=gamma)\n",
    "schedulers = [scheduler_warmup, scheduler_decay]\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers,\n",
    "    milestones=[10])\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    acc = 0.\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y = batch['y']\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        acc += sum(torch.argmax(out, dim=-1) == y)\n",
    "    acc = acc/len(dataset)\n",
    "    scheduler.step()\n",
    "    print(f'[epoch: {epoch}][loss/acc: {np.mean(losses):.4f}/{acc:.4f}]')\n",
    "    \n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, 'model-checkpoint-stage-1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Finetuning\n",
    "Now that we pre-trained a model to recognize word structures and grammar, we finetune it on a more realistic data distribution (with more than one blanks in words) as generated by ```HangmanDatasetStage2```. Now, the objective function is to guess all of the letters that are masked out in the input, so we use a multi-label BCE loss. In addition, we utilize the fact that none of the revealed letters can be masked and effectively \"mask\" them out in the output by using ```fill_mask``` returned by the tokenizer. To pick the best model during training, we perform validation using a hold-out dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dictionary, verbose=False, n=100):\n",
    "    successes = 0.\n",
    "    words = np.random.choice(dictionary, size=n, replace=False)\n",
    "    for word in tqdm(words):\n",
    "        model.flush()\n",
    "        pattern = ['.']*len(word)\n",
    "        if verbose:\n",
    "            print(f'[GAME]: {\"\".join(pattern)}')\n",
    "        missed = 0\n",
    "        while True:\n",
    "            letter = model.guess(\"\".join(pattern))\n",
    "            guess_correct = False\n",
    "            for i in range(len(word)):\n",
    "                if word[i] == letter:\n",
    "                    pattern[i] = letter\n",
    "                    guess_correct = True\n",
    "            if not guess_correct:\n",
    "                missed += 1\n",
    "            if missed == 6:\n",
    "                if verbose:\n",
    "                    print(f'[FAIL][word: {word}]')\n",
    "                break\n",
    "            elif '.' not in pattern:\n",
    "                successes += 1\n",
    "                if verbose:\n",
    "                    print(f'[OK!][word: {word}]')\n",
    "                break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'-[{6-missed}]: {\"\".join(pattern)} ({letter})')\n",
    "    return successes/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:32<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0][loss/acc: 0.3370/0.3300]\n",
      "[epoch: 1][loss/acc: 0.2825/0.3300]\n",
      "[epoch: 2][loss/acc: 0.2728/0.3300]\n",
      "[epoch: 3][loss/acc: 0.2681/0.3300]\n",
      "[epoch: 4][loss/acc: 0.2648/0.3300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:44<00:00,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5][loss/acc: 0.2623/0.5600]\n",
      "[epoch: 6][loss/acc: 0.2601/0.5600]\n",
      "[epoch: 7][loss/acc: 0.2586/0.5600]\n",
      "[epoch: 8][loss/acc: 0.2569/0.5600]\n",
      "[epoch: 9][loss/acc: 0.2553/0.5600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:38<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10][loss/acc: 0.2547/0.6200]\n",
      "[epoch: 11][loss/acc: 0.2535/0.6200]\n",
      "[epoch: 12][loss/acc: 0.2529/0.6200]\n",
      "[epoch: 13][loss/acc: 0.2512/0.6200]\n",
      "[epoch: 14][loss/acc: 0.2505/0.6200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:37<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 15][loss/acc: 0.2497/0.6600]\n",
      "[epoch: 16][loss/acc: 0.2498/0.6600]\n",
      "[epoch: 17][loss/acc: 0.2491/0.6600]\n",
      "[epoch: 18][loss/acc: 0.2490/0.6600]\n",
      "[epoch: 19][loss/acc: 0.2484/0.6600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:42<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 20][loss/acc: 0.2480/0.6000]\n",
      "[epoch: 21][loss/acc: 0.2483/0.6000]\n",
      "[epoch: 22][loss/acc: 0.2484/0.6000]\n",
      "[epoch: 23][loss/acc: 0.2472/0.6000]\n",
      "[epoch: 24][loss/acc: 0.2475/0.6000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:35<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 25][loss/acc: 0.2471/0.7200]\n",
      "[epoch: 26][loss/acc: 0.2466/0.7200]\n",
      "[epoch: 27][loss/acc: 0.2467/0.7200]\n",
      "[epoch: 28][loss/acc: 0.2463/0.7200]\n",
      "[epoch: 29][loss/acc: 0.2469/0.7200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:38<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 30][loss/acc: 0.2462/0.6700]\n",
      "[epoch: 31][loss/acc: 0.2461/0.6700]\n",
      "[epoch: 32][loss/acc: 0.2459/0.6700]\n",
      "[epoch: 33][loss/acc: 0.2460/0.6700]\n",
      "[epoch: 34][loss/acc: 0.2459/0.6700]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:41<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 35][loss/acc: 0.2460/0.6600]\n",
      "[epoch: 36][loss/acc: 0.2455/0.6600]\n",
      "[epoch: 37][loss/acc: 0.2453/0.6600]\n",
      "[epoch: 38][loss/acc: 0.2452/0.6600]\n",
      "[epoch: 39][loss/acc: 0.2454/0.6600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:44<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 40][loss/acc: 0.2452/0.7100]\n",
      "[epoch: 41][loss/acc: 0.2450/0.7100]\n",
      "[epoch: 42][loss/acc: 0.2455/0.7100]\n",
      "[epoch: 43][loss/acc: 0.2454/0.7100]\n",
      "[epoch: 44][loss/acc: 0.2453/0.7100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [05:04<02:37,  4.62s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for dimension 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 37\u001b[0m     acc \u001b[38;5;241m=\u001b[39m validate(api, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m acc:\n\u001b[1;32m     39\u001b[0m         best_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(api, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n)):\n\u001b[1;32m      4\u001b[0m     api\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m----> 5\u001b[0m     acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mstart_game(practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\u001b[38;5;241m/\u001b[39mn\n",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully start a new game! Game ID: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m. # of tries remaining: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Word: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(game_id, tries_remains, word))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tries_remains\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# get guessed letter from user code\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     guess_letter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguess(word)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# append guessed letter to guessed letters field in hangman object\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters\u001b[38;5;241m.\u001b[39mappend(guess_letter)\n",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m, in \u001b[0;36mHangmanAPI.guess\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mguess\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m     29\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m word[::\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     letter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mguess(pattern)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m letter\n",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m, in \u001b[0;36mHangmanTransformer.guess\u001b[0;34m(self, pattern)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mguess\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern):\n\u001b[0;32m---> 62\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(pattern)\n\u001b[1;32m     63\u001b[0m     inp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m     inp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mHangmanTokenizer.__call__\u001b[0;34m(self, pattern, word)\u001b[0m\n\u001b[1;32m     19\u001b[0m     fill_mask[\u001b[38;5;28mord\u001b[39m(pattern[j])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m97\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     inp[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_idxs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m         label[\u001b[38;5;28mord\u001b[39m(word[j])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m97\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for dimension 0 with size 20"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('model-checkpoint-stage-1.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "gamma = (1./50)**(1./50)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.bert_model.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}],\n",
    "    lr=1e-4)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=10)\n",
    "scheduler_decay = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=gamma)\n",
    "schedulers = [scheduler_warmup, scheduler_decay]\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers,\n",
    "    milestones=[10])\n",
    "dataset = HangmanDatasetStage2(train_dictionary, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "best_model = {\n",
    "    'accuracy': acc,\n",
    "    'model': None}\n",
    "for epoch in range(50):\n",
    "    losses = []\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        acc = validate(model, test_dictionary, n=100)\n",
    "        if best_model['accuracy'] < acc:\n",
    "            best_model['model'] = model.state_dict()\n",
    "        torch.save(best_model['model'], 'model-checkpoint-stage-2.pt')\n",
    "        model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        out = out + batch['fill_mask']\n",
    "        loss = criterion(out, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    scheduler.step()\n",
    "    print(f'[epoch: {epoch}][loss/acc: {np.mean(losses):.4f}/{acc:.4f}]')\n",
    "    \n",
    "model.eval()\n",
    "acc = validate(api, n=100)\n",
    "if best_model['accuracy'] < acc:\n",
    "    best_model['model'] = model.state_dict()\n",
    "torch.save(best_model['model'], 'model-checkpoint-stage-2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbose Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model ready]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = HangmanTokenizer(max_seq_len=20, device=device)\n",
    "final_model = HangmanTransformer(\n",
    "    max_seq_len=20,\n",
    "    model_dim=512,\n",
    "    latent_dim=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_heads=8,\n",
    "    num_layers=12,\n",
    "    vocab_size=26+len(tokenizer.special_idxs))\n",
    "state_dict = torch.load('model-checkpoint-stage-2.pt')\n",
    "final_model.load_state_dict(state_dict)\n",
    "final_model.eval()\n",
    "print(f'[Model ready]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:01<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[accuracy: 0.592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple eval.\n",
    "\n",
    "N = 500\n",
    "word = np.random.choice(test_dictionary, size=N, replace=False)\n",
    "acc = validate(final_model, word, verbose=False, n=N)\n",
    "print(f'[accuracy: {acc:.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAME]: .........\n",
      "-[6]: .e..e...e (e)\n",
      "-[6]: .e.re...e (r)\n",
      "-[6]: .e.re..te (t)\n",
      "-[6]: .e.re.ate (a)\n",
      "-[5]: .e.re.ate (c)\n",
      "-[4]: .e.re.ate (p)\n",
      "-[3]: .e.re.ate (d)\n",
      "-[3]: .egregate (g)\n",
      "[OK!][word: segregate]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Single eval.\n",
    "\n",
    "word = np.random.choice(test_dictionary)\n",
    "acc = validate(final_model, [word], verbose=True, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
